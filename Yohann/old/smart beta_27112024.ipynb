{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T12:12:34.206076Z",
     "iopub.status.busy": "2024-10-26T12:12:34.205030Z",
     "iopub.status.idle": "2024-10-26T12:13:13.281267Z",
     "shell.execute_reply": "2024-10-26T12:13:13.279864Z",
     "shell.execute_reply.started": "2024-10-26T12:12:34.206032Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install yfinance yesg PyPortfolioOpt portfolio-backtest riskfolio-lib plotly scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T12:14:25.220698Z",
     "iopub.status.busy": "2024-10-26T12:14:25.220296Z",
     "iopub.status.idle": "2024-10-26T12:14:25.230782Z",
     "shell.execute_reply": "2024-10-26T12:14:25.229078Z",
     "shell.execute_reply.started": "2024-10-26T12:14:25.220661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import yesg\n",
    "#import plotly.express as px\n",
    "#import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "#from plotly.subplots import make_subplots\n",
    "#import riskfolio as rp\n",
    "import warnings\n",
    "import requests\n",
    "from io import BytesIO, StringIO\n",
    "import pypfopt as pf\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equal risk portfolio weight optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the cost of transactions\n",
    "def transaction_costs(w, w0, TC):\n",
    "    return np.sum(TC * np.abs(w - w0)**2)\n",
    "\n",
    "# Modified objective function to include transaction costs\n",
    "def objective(w, cov_mat, w0, TC, factor_TC):\n",
    "    # Original variance-based function\n",
    "    s = 0\n",
    "    for i in range(len(w)):\n",
    "        for j in range(len(w)):\n",
    "            s += (w[i] * (cov_mat @ w)[i] - w[j] * (cov_mat @ w)[j])**2\n",
    "    # Add transaction costs as a penalty term\n",
    "    return s + factor_TC * transaction_costs(w, w0, TC)\n",
    "    \n",
    "def rp_weights(df, w0= np.array([0.2, 0.2, 0.2, 0.2, 0.2]), TC = np.array([0.0005, 0.0005, 0.0005, 0.0005, 0.0005]), factor_TC = .0001):\n",
    "    \"\"\"\n",
    "    Function to calculate the weights for the risk parity portfolio\n",
    "    df: DataFrame with the ETF returns\n",
    "    w0: Initial weights (current portfolio)\n",
    "    TC: Transaction costs (5 bps)\n",
    "    factor_TC: Scaling factor for transaction costs (tune this as needed)\n",
    "    \"\"\"\n",
    "    cov_mat = df.cov().values  # Covariance matrix of ETF returns\n",
    "\n",
    "    # Constraints: weights sum to 1 and are long-only\n",
    "    constraints = [{'type': 'eq', 'fun': lambda w: np.sum(w) - 1},  # Sum of weights = 1\n",
    "                   {'type': 'ineq', 'fun': lambda w: w}]            # Weights must be positive (long only)\n",
    "\n",
    "    # Optimize\n",
    "    result = opt.minimize(objective, w0, args=(cov_mat, w0, TC, factor_TC), constraints=constraints, tol=1e-20)\n",
    "    w_opt = result.x  # Optimized weights\n",
    "    return w_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimum variance portfolio weight optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_var_weights(train_data):\n",
    "    \"\"\"Effectue l'optimisation Min-Var et retourne les poids optimaux.\"\"\"\n",
    "    try:\n",
    "        mean_returns = train_data.mean()\n",
    "        cov_matrix = train_data.cov()\n",
    "        ef = pf.EfficientFrontier(mean_returns, cov_matrix)\n",
    "        weights = ef.min_volatility()\n",
    "        return np.array(list(weights.values()))\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du calcul des poids du min-var portfolio : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(portfolio_returns, spy_returns, slice):\n",
    "    \"\"\"Calcule les métriques de performance pour une tranche donnée.\"\"\"\n",
    "    try:\n",
    "        cumulative_return = float((1 + portfolio_returns).prod() - 1)\n",
    "        cov_matrix = np.cov(portfolio_returns, spy_returns)\n",
    "        beta = cov_matrix[0, 1] / cov_matrix[1, 1]\n",
    "        expected_annual_return = float(portfolio_returns.mean() * 252)\n",
    "        volatility = float(portfolio_returns.std() * np.sqrt(i))\n",
    "        sharpe_ratio = float(cumulative_return / volatility)\n",
    "        treynor_ratio = float(cumulative_return / beta)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"Slice\": slice,\n",
    "            \"Expected Annual Return\": expected_annual_return,\n",
    "            \"Volatility\": volatility,\n",
    "            \"Sharpe Ratio\": sharpe_ratio,\n",
    "            \"Treynor Ratio\": treynor_ratio,\n",
    "            \"Cumulative Return\": cumulative_return\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du calcul des métriques de la tranche {i} : {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(tickers, start_date, end_date):\n",
    "    try:\n",
    "        data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
    "        returns = data.pct_change().dropna()\n",
    "\n",
    "        spy_data = yf.download('SPY', start=start_date, end=end_date)['Adj Close']\n",
    "        spy_returns = spy_data.pct_change().dropna()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la récupération des données : {e}\")\n",
    "        return\n",
    "    return returns, spy_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_metrics(metrics_list):    \n",
    "    try:\n",
    "        metrics_df = pd.DataFrame(metrics_list)\n",
    "        metrics_df = metrics_df.set_index(\"Slice\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Erreur lors de la conversion en DataFrame : {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur inattendue : {e}\")\n",
    "        return\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(i, j, k, start_date, end_date, tickers, strategy):\n",
    "\n",
    "  # Désactiver les warnings pour les matrices non définies positives\n",
    "    warnings.filterwarnings(\"ignore\", message=\"You must convert self.cov to a positive definite matrix\")\n",
    "    try:\n",
    "        # Vérification des paramètres d'entrée\n",
    "        if j < i:\n",
    "            raise ValueError(\"Le deuxième paramètre (j) doit être supérieur au premier (i).\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Erreur d'entrée : {e}\")\n",
    "        return\n",
    "\n",
    "    # Étape 1 : Récupération des données\n",
    "    returns, spy_returns = get_data(tickers, start_date, end_date)\n",
    "\n",
    "    # Étape 2 : Initialisation des variables\n",
    "    try:\n",
    "        train_data = returns.iloc[:j, :]\n",
    "        test_data = returns.iloc[j:, :]\n",
    "        spy_test_data = spy_returns.iloc[j:]\n",
    "        metrics_list = []\n",
    "        start_idx = 0\n",
    "        weights_list = []\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'initialisation des variables : {e}\")\n",
    "        return\n",
    "\n",
    "    # Étape 3 : Optimisation RP pour chaque tranche\n",
    "    try:\n",
    "        while start_idx + i <= len(test_data) and start_idx + i <= len(spy_test_data) :\n",
    "            # Etape 3 : Découpage des données de test\n",
    "            slice_data = test_data.iloc[start_idx:start_idx + i]\n",
    "            spy_slice_data = spy_test_data.iloc[start_idx:start_idx + i].to_numpy().flatten()\n",
    "\n",
    "            # Etape 4 : Calcul des poids du portefeuille\n",
    "            if strategy==\"ERC\":\n",
    "                # Optimisation RP sur les données d'entraînement actuelles\n",
    "                #port = rp.Portfolio(returns=train_data)\n",
    "                #port.assets_stats(method_mu='hist', method_cov='hist')\n",
    "                #weights = port.rp_optimization(model='Classic', rm='MV').to_numpy().flatten()\n",
    "                weights = rp_weights(train_data)\n",
    "                weights_list.append(weights)\n",
    "                portfolio_returns = slice_data @ weights    # Calcul des rendements du portefeuille pour la tranche\n",
    "            \n",
    "            elif strategy==\"Equal weight\":\n",
    "                weights = np.array([1/len(tickers)]*len(tickers))\n",
    "                weights_list.append(weights)\n",
    "                portfolio_returns = slice_data @ weights\n",
    "            \n",
    "            elif strategy==\"Benchmark\":\n",
    "                portfolio_returns = spy_slice_data\n",
    "\n",
    "            elif strategy==\"Min-var\":\n",
    "                weights = min_var_weights(train_data)\n",
    "                weights_list.append(weights)\n",
    "                portfolio_returns = slice_data @ weights\n",
    "\n",
    "            else:\n",
    "                print(\"Stratégie non reconnue, veuillez choisir entre 'ERC', 'Equal weight', 'Benchmark' et 'Min-var'\")\n",
    "                return\n",
    "\n",
    "            # Étape 5 : Calcul des métriques\n",
    "            metrics_list.append(calculate_metrics(portfolio_returns, spy_slice_data, len(metrics_list) + 1))\n",
    "\n",
    "            # Mise à jour des données d'entraînement\n",
    "            train_data = pd.concat([train_data, test_data.iloc[start_idx:start_idx + k]]).iloc[-j:]\n",
    "            start_idx += k\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'optimisation ou du calcul des métriques : {e}\")\n",
    "        return\n",
    "\n",
    "    # Étape 6 : Conversion en DataFrame\n",
    "    metrics_df = convert_metrics(metrics_list)\n",
    "\n",
    "    return metrics_df, weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['SPHQ', 'IVE', 'SPYD', 'SPLV', 'SPMO']\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2024-11-01'\n",
    "i = 5\n",
    "j = 22 \n",
    "k = i\n",
    "strategies = [\"Equal weight\", \"Benchmark\", \"Min-var\", \"ERC\"]\n",
    "sharpe_ratios = pd.DataFrame(columns=strategies)\n",
    "cumulative_returns = pd.DataFrame(columns=strategies)\n",
    "for strategy in strategies:\n",
    "    metrics_df, weights_list = backtest(i, j, k, start_date, end_date, tickers, strategy)\n",
    "    sharpe_ratios[strategy] = metrics_df[\"Sharpe Ratio\"]\n",
    "    cumulative_returns[strategy] = metrics_df[\"Cumulative Return\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(data):\n",
    "    # Create a figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Loop through each column in the DataFrame and add a trace for each\n",
    "    for column in data.columns:\n",
    "        fig.add_trace(go.Scatter(x=data.index, y=data[column], mode='lines', name=column))\n",
    "\n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "\n",
    "def box_plot_data(data):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for column in data.columns:\n",
    "        fig.add_trace(go.Box(y=data[column], name=column))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Box Plot\",\n",
    "        yaxis_title=\"Values\",\n",
    "        xaxis_title=\"Columns\",\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def plot_moving_average_data(data):\n",
    "    # Apply a rolling mean (moving average) for smoothing\n",
    "    smoothed_sr = data.rolling(window=30, min_periods=1).mean()  # 5-period rolling average\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for column in smoothed_sr.columns:\n",
    "        fig.add_trace(go.Scatter(x=smoothed_sr.index, y=smoothed_sr[column], mode='lines', name=column))\n",
    "\n",
    "    # Update layout for better visualization\n",
    "    fig.update_layout(\n",
    "        title=\"Smoothed Trends of Each Column\",\n",
    "        xaxis_title=\"Index\",\n",
    "        yaxis_title=\"Smoothed Values\",\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpe_ratios.describe()\n",
    "plot_data(sharpe_ratios)\n",
    "box_plot_data(sharpe_ratios)\n",
    "plot_moving_average_data(sharpe_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns.describe()\n",
    "plot_data(cumulative_returns)\n",
    "box_plot_data(cumulative_returns)\n",
    "plot_moving_average_data(cumulative_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From ETFs weights to stocks weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-26T12:15:06.648394Z",
     "iopub.status.busy": "2024-10-26T12:15:06.647965Z",
     "iopub.status.idle": "2024-10-26T12:15:06.666655Z",
     "shell.execute_reply": "2024-10-26T12:15:06.665298Z",
     "shell.execute_reply.started": "2024-10-26T12:15:06.648351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def download_data(url):\n",
    "    \"\"\"\n",
    "    Télécharge et retourne les données d'une URL, en gérant les fichiers Excel et CSV de manière appropriée.\n",
    "    \"\"\"\n",
    "    if url.endswith('.xls') or url.endswith('.xlsx'):\n",
    "        skip_rows = 4\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = pd.read_excel(BytesIO(response.content), skiprows=skip_rows)\n",
    "    else:\n",
    "        skip_rows = 0\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        csv_data = response.content.decode('utf-8')\n",
    "        try:\n",
    "            data = pd.read_csv(StringIO(csv_data), skiprows=skip_rows)\n",
    "        except pd.errors.ParserError:\n",
    "            try:\n",
    "                data = pd.read_csv(StringIO(csv_data), delimiter=';', skiprows=skip_rows)\n",
    "            except pd.errors.ParserError:\n",
    "                data = pd.read_csv(StringIO(csv_data), skiprows=9 + skip_rows)\n",
    "\n",
    "    if not any('weight' in col.lower() for col in data.columns):\n",
    "        data = pd.read_csv(StringIO(csv_data), skiprows=9 + skip_rows)\n",
    "    return data\n",
    "\n",
    "def normalize_ticker(ticker):\n",
    "    \"\"\"\n",
    "    Normalise le ticker en supprimant les espaces et les caractères inutiles.\n",
    "    \"\"\"\n",
    "    if pd.isna(ticker):  # Vérifier si ticker est NaN\n",
    "        return ''\n",
    "    return ticker.replace('/', '').strip().upper()\n",
    "\n",
    "def calculate_total_weights_per_etf(url, etf_weight):\n",
    "    \"\"\"\n",
    "    Calcule le poids total pour chaque action d'un fichier CSV ou Excel.\n",
    "    \"\"\"\n",
    "    data = download_data(url)\n",
    "    weight_column = next((col for col in data.columns if 'weight' in col.lower()), None)\n",
    "    if weight_column is None:\n",
    "        print(\"Colonne 'Weight' ou 'Weight (%)' introuvable dans le fichier.\")\n",
    "        return None\n",
    "\n",
    "    holding_ticker_column = next((col for col in data.columns if 'holding ticker' in col.lower()), None)\n",
    "    ticker_column = next((col for col in data.columns if 'ticker' in col.lower()), None)\n",
    "    if holding_ticker_column:\n",
    "        ticker_column = holding_ticker_column\n",
    "    elif not ticker_column:\n",
    "        print(\"Aucune colonne 'Ticker' ou 'Holding Ticker' introuvable dans le fichier.\")\n",
    "        return None\n",
    "\n",
    "    data['Weighted_Weight'] = data[weight_column] * etf_weight\n",
    "\n",
    "    if 'Name' in data.columns:\n",
    "        data = data[~data['Name'].isin([\"US DOLLAR\", \"Cash/Receivables/Payables\", \"S+P500 EMINI FUT  DEC24\", \"CASH COLLATERAL MSFUT USD\", \"USD CASH\", \"SSI US GOV MONEY MARKET CLASS\"])]\n",
    "\n",
    "    # Normaliser les tickers\n",
    "    data[ticker_column] = data[ticker_column].apply(normalize_ticker)\n",
    "    \n",
    "    total_weights = data.groupby(ticker_column)[['Name', 'Weighted_Weight']].sum().reset_index()\n",
    "    total_weights.columns = ['Ticker', 'Name', 'Total_Weight']\n",
    "    total_weights = total_weights[total_weights['Total_Weight'] > 0]\n",
    "    return total_weights\n",
    "\n",
    "def calculate_total_weights(weights):\n",
    "    # Téléchargement et traitement des données pour chaque URL\n",
    "    urls = [\n",
    "        \"https://www.invesco.com/us/financial-products/etfs/holdings/main/holdings/0?audienceType=Investor&action=download&ticker=SPHQ\",\n",
    "        \"https://www.invesco.com/us/financial-products/etfs/holdings/main/holdings/0?audienceType=Investor&action=download&ticker=SPLV\",\n",
    "        \"https://www.invesco.com/us/financial-products/etfs/holdings/main/holdings/0?audienceType=Investor&action=download&ticker=SPMO\",\n",
    "        \"https://www.ishares.com/us/products/239728/ishares-sp-500-value-etf/1467271812596.ajax?fileType=csv&fileName=IVE_holdings&dataType=fund\",\n",
    "        \"https://www.ssga.com/us/en/intermediary/library-content/products/fund-data/etfs/us/holdings-daily-us-en-spyd.xlsx\"\n",
    "    ]\n",
    "\n",
    "    final_data = pd.DataFrame()\n",
    "\n",
    "    # Appliquer chaque URL et poids d'ETF\n",
    "    for url, weight in zip(urls, weights):\n",
    "        etf_data = calculate_total_weights_per_etf(url, weight)\n",
    "        if etf_data is not None:\n",
    "            final_data = pd.concat([final_data, etf_data])\n",
    "\n",
    "    # Étape finale : Regrouper par ticker exact et sommer les poids\n",
    "    final_data = final_data.groupby('Ticker', as_index=False)['Total_Weight'].sum()\n",
    "    final_data['Total_Weight'] /= final_data['Total_Weight'].sum()\n",
    "\n",
    "    # Trier les données en ordre décroissant de poids\n",
    "    #final_data.sort_values(by='Total_Weight', ascending=False, inplace=True)\n",
    "\n",
    "    # Vérification des doublons\n",
    "    duplicate_tickers = final_data[final_data.duplicated(subset='Ticker', keep=False)]\n",
    "    if not duplicate_tickers.empty:\n",
    "        print(\"Doublons trouvés dans les tickers :\")\n",
    "        print(duplicate_tickers)\n",
    "    #else:\n",
    "    #    print(\"Aucun doublon trouvé dans les tickers.\")\n",
    "\n",
    "    # Affichage du résultat final\n",
    "    #print(final_data.to_string(index=False))\n",
    "\n",
    "    # Enregistrer dans un fichier CSV\n",
    "    #final_data.to_csv(\"poids_actions_etf.csv\", index=False)\n",
    "    #print(\"Les données complètes ont été enregistrées dans 'poids_actions_etf.csv'.\")\n",
    "\n",
    "    final_data.columns = ['Symbol', 'Weights']\n",
    "    final_data = final_data.set_index('Symbol')\n",
    "    \n",
    "    return final_data\n",
    "\n",
    "#Stocks with several classes: fox and news corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_weights = calculate_total_weights(weights_list[-1])\n",
    "stock_weights.sort_values(by='Weights', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We do not take into account the worst stocks in terms of ESG scores\n",
    "- Sector constraints (the s&p is already concentrated so it might be interesting to allow a difference with the sector wiehgts in the original s&p 500)\n",
    "- Concentration limits (a limit for each stock)\n",
    "- Liquidity constraints (use only stocks with volume higher than a threshold): not so interesting because all s&p 500 stocks are liquid\n",
    "- no short allowed\n",
    "- tracking error: determined with backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESG constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esg_constraints(stock_weights, quantile_threshold):\n",
    "    \"\"\"\n",
    "    quantile_threshold: float between 0 and 1 to keep the best quantile_threshold quantile of the stocks\n",
    "    stock_weights: DataFrame with the weights of the stocks in the portfolio\n",
    "    \"\"\"\n",
    "    # get the s&p 500 tickers with their name, sector and sub-industry\n",
    "    df_sp500 = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')[0][['Symbol', 'Security', 'GICS Sector', 'GICS Sub-Industry']]\n",
    "    df_sp500 = df_sp500.set_index('Symbol') # set the index to be the symbol\n",
    "\n",
    "    df_sp500.drop(index='GOOG', inplace=True) # drop the GOOG row because there is already a GOOGL row\n",
    "\n",
    "    df_esg = df_sp500.copy()\n",
    "    for ticker in df_sp500.index:\n",
    "        try:\n",
    "            df_esg.loc[ticker, \"ESG Score\"] = yesg.get_historic_esg(ticker).iloc[-1,0]\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    #drop all the stocks that have no ESG score\n",
    "    df_esg.dropna(axis=0, inplace=True)\n",
    "\n",
    "    #The best ESG score is 0\n",
    "    #drop the worst stocks in terms of ESG score\n",
    "    df_esg = df_esg[df_esg['ESG Score'] < df_esg['ESG Score'].quantile(quantile_threshold)]\n",
    "\n",
    "    #drop all the stocks that have an ESG score above the threshold\n",
    "    #threshold = 30\n",
    "    #df_esg = df_esg[df_esg['ESG Score'] < threshold]\n",
    "\n",
    "    df_esg = df_esg.merge(stock_weights, left_index=True, right_index=True)\n",
    "\n",
    "    df_esg['Weights'] /= df_esg['Weights'].sum()\n",
    "\n",
    "    return df_esg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_threshold = 0.9\n",
    "df_esg = esg_constraints(stock_weights, quantile_threshold)\n",
    "df_esg.sort_values(by='Weights', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sector_constraints(df, sector_threshold):\n",
    "    \"\"\"\n",
    "    df: DataFrame following the same format as the one returned by esg_constraints\n",
    "    sector_threshold: float between 0 and 1 that a sector cannot exceed in the portfolio\n",
    "    \"\"\"\n",
    "    #We calculate the weights of the stocks in each sector\n",
    "    df_weights_by_sector = df[['GICS Sector', 'Weights']].groupby('GICS Sector').sum()\n",
    "    \n",
    "    #We create a list of the sectors that have a weight above the threshold\n",
    "    sectors_above_threshold = df_weights_by_sector.loc[df_weights_by_sector['Weights'] > sector_threshold].index\n",
    "    while len(sectors_above_threshold) > 0:\n",
    "        for sector in sectors_above_threshold:\n",
    "            #for each stock in the sector, we apply a factor such that the sum of all stocks in this sector is equal to the threshold\n",
    "            df_sector = df[df['GICS Sector'] == sector]\n",
    "            factor = sector_threshold / df_weights_by_sector.loc[sector, 'Weights']\n",
    "            df.loc[df_sector.index, 'Weights'] = df_sector['Weights'] * factor\n",
    "        \n",
    "        #we increase the weights of the stocks in the other sectors to keep the sum of the weights equal to 1\n",
    "        df.loc[~df['GICS Sector'].isin(sectors_above_threshold), \"Weights\"] /= df.loc[~df['GICS Sector'].isin(sectors_above_threshold), \"Weights\"].sum() / (1-sector_threshold*len(sectors_above_threshold))\n",
    "        \n",
    "        #Some sectors may now have a weight above the threshold so we repeat the process\n",
    "        df_weights_by_sector = df[['GICS Sector', 'Weights']].groupby('GICS Sector').sum()\n",
    "        sectors_above_threshold = df_weights_by_sector.loc[df_weights_by_sector['Weights'] > sector_threshold].index\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weights</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GICS Sector</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Communication Services</th>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consumer Discretionary</th>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Financials</th>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health Care</th>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Industrials</th>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Information Technology</th>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Materials</th>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real Estate</th>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Utilities</th>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Weights\n",
       "GICS Sector                    \n",
       "Communication Services     0.06\n",
       "Consumer Discretionary     0.06\n",
       "Financials                 0.12\n",
       "Health Care                0.17\n",
       "Industrials                0.12\n",
       "Information Technology     0.17\n",
       "Materials                  0.12\n",
       "Real Estate                0.06\n",
       "Utilities                  0.12"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sector_threshold = 0.17\n",
    "weights_after_sector = sector_constraints(df_esg, sector_threshold)\n",
    "\n",
    "#Check that none of the sectors have a weight above the threshold\n",
    "#weights_after_sector[['GICS Sector', 'Weights']].groupby('GICS Sector').sum()\n",
    "\n",
    "#Check that the sum of the weights is equal to 1\n",
    "#weights_after_sector[['GICS Sector', 'Weights']].groupby('GICS Sector').sum().sum()\n",
    "\n",
    "weights_after_sector.sort_values(by='Weights', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stock_constraints(df, stock_threshold):\n",
    "    \"\"\"\n",
    "    df: DataFrame following the same format as the one returned by sector_constraints\n",
    "    stock_threshold: float between 0 and 1 that a stock cannot exceed in the portfolio\n",
    "    \"\"\"\n",
    "    #We create a list of the stocks that have a weight above the threshold\n",
    "    stocks_above_threshold = df.loc[df['Weights'] > stock_threshold].index\n",
    "    while len(stocks_above_threshold) > 0:\n",
    "        #we set the weights of the stocks above the threshold to the threshold\n",
    "        df.loc[stocks_above_threshold, 'Weights'] = stock_threshold\n",
    "        #we increase (proportionally) the weights of the other stocks to keep the sum of the weights equal to 1\n",
    "        df.loc[~df.index.isin(stocks_above_threshold), \"Weights\"] /= df.loc[~df.index.isin(stocks_above_threshold), \"Weights\"].sum() / (1-stock_threshold*len(stocks_above_threshold))\n",
    "        #Some stocks may now have a weight above the threshold so we repeat the process\n",
    "        stocks_above_threshold = df.loc[df['Weights'] > stock_threshold].index\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Max weight for a stock before stock constraints: \", weights_after_sector['Weights'].max())\n",
    "stock_threshold = 0.05\n",
    "weights_after_stock = stock_constraints(weights_after_sector, stock_threshold)\n",
    "#print(\"Max weight for a stock after stock constraints: \", weights_after_stock['Weights'].max())\n",
    "\n",
    "#Check that none of the stocks have a weight above the threshold\n",
    "#weights_after_stock['Weights'].max()\n",
    "\n",
    "#Check that the sum of the weights is equal to 1\n",
    "#weights_after_stock['Weights'].sum()\n",
    "\n",
    "weights_after_stock.sort_values(by='Weights', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liquidity constraints\n",
    "We can limit the universe by only taking into account stocks with a volume higher than a threshold but I'm not sure it's necessary because all s&p 500 stocks are liquid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking error constraints\n",
    "Apart from using weights closer to the original s&p 500 weights, I don't really see how to reduce the tracking error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "### Implementations of equal risk contribution\n",
    "- https://github.com/matthewgilbert/erc/blob/master/erc/erc.py\n",
    "- https://github.com/mirca/riskparity.py (not used)\n",
    "- https://thequantmba.wordpress.com/2016/12/14/risk-parityrisk-budgeting-portfolio-in-python/\n",
    "\n",
    "### Papers\n",
    "- [Paper of Maillard, Roncalli and Teiletche](http://thierry-roncalli.com/download/erc.pdf)\n",
    "- [Slides of Maillard, Roncalli and Teiletche](http://www.thierry-roncalli.com/download/erc-slides.pdf)\n",
    "- [Master's thesis of David Stefanovits](https://ethz.ch/content/dam/ethz/special-interest/math/risklab-dam/documents/walter-saxer-preis/ma-stefanovits.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
